{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sl', 'es', 'el', 'nl', 'hu', 'it', 'bg', 'sk', 'da', 'sv', 'cs', 'lt', 'de', 'en', 'pl', 'fr', 'fi', 'lv', 'pt', 'et', 'ro']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/lang-detect/txt/\"\n",
    "dir_list = os.listdir(data_path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) > 1:\n",
    "            return lines[1].strip(\"\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 1.8 s, total: 4.86 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data, labels = [], []\n",
    "for dir_name in dir_list:\n",
    "    files_list = os.listdir(data_path + dir_name)\n",
    "    for f in files_list:\n",
    "        sent = read_data(data_path + dir_name + \"/\" + f)\n",
    "        if sent:\n",
    "            data.append(sent)\n",
    "            labels.append(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of data', 186458)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of data\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Corrections et intentions de vote: voir proc\\xc3\\xa8s-verbal', 'fr')\n",
      "('Pr\\xc3\\xadprava zasadnutia Eur\\xc3\\xb3pskej rady (18. a 19. j\\xc3\\xbana 2009) (rozprava) ', 'sk')\n",
      "('Zah\\xc3\\xa1jen\\xc3\\xad zased\\xc3\\xa1n\\xc3\\xad', 'cs')\n",
      "('1. \\xc3\\x84ndring av de fiskem\\xc3\\xb6jligheter och den ekonomiska ers\\xc3\\xa4ttning som f\\xc3\\xb6reskrivs i avtalet mellan EG och Seychellerna (', 'sv')\n",
      "('P\\xc3\\xadsemn\\xc3\\xa1 prohl\\xc3\\xa1\\xc5\\xa1en\\xc3\\xad obsa\\xc5\\xbeen\\xc3\\xa1 v seznamu (\\xc4\\x8dl\\xc3\\xa1nek 123): viz z\\xc3\\xa1pis', 'cs')\n",
      "('2. Instrument til finansiering af udviklingssamarbejde (', 'da')\n",
      "('Seuraavan istunnon esityslista: ks. p\\xc3\\xb6yt\\xc3\\xa4kirja', 'fi')\n",
      "('Dichiarazioni scritte che figurano nel registro (articolo 116 del Regolamento): vedasi processo verbale', 'it')\n",
      "('Muntliga fr\\xc3\\xa5gor och skriftliga f\\xc3\\xb6rklaringar (ingivande): se protokollet', 'sv')\n",
      "('6. Stan zaawansowania SIS II i VIS (g\\xc5\\x82osowanie) ', 'pl')\n",
      "(\"5. Zone couverte par l'accord de la Commission g\\xc3\\xa9n\\xc3\\xa9rale des p\\xc3\\xaaches pour la M\\xc3\\xa9diterran\\xc3\\xa9e (CGPM) (\", 'fr')\n",
      "('\\xd0\\x95\\xd0\\xb4\\xd0\\xbd\\xd0\\xbe\\xd0\\xbc\\xd0\\xb8\\xd0\\xbd\\xd1\\x83\\xd1\\x82\\xd0\\xbd\\xd0\\xb8 \\xd0\\xb8\\xd0\\xb7\\xd0\\xba\\xd0\\xb0\\xd0\\xb7\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f \\xd0\\xbf\\xd0\\xbe \\xd0\\xb2\\xd1\\x8a\\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x81\\xd0\\xb8 \\xd1\\x81 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd0\\xb8\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xb0 \\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb8\\xd0\\xbc\\xd0\\xbe\\xd1\\x81\\xd1\\x82', 'bg')\n",
      "('Votare', 'ro')\n",
      "('Agenda for next sitting: see Minutes', 'en')\n",
      "('Prohl\\xc3\\xa1\\xc5\\xa1en\\xc3\\xad p\\xc5\\x99edsedy', 'cs')\n",
      "('2. \\xd0\\x9f\\xd1\\x80\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd0\\xba\\xd0\\xbe\\xd0\\xbb \\xd0\\xba\\xd1\\x8a\\xd0\\xbc \\xd0\\x95\\xd0\\xb2\\xd1\\x80\\xd0\\xbe-\\xd1\\x81\\xd1\\x80\\xd0\\xb5\\xd0\\xb4\\xd0\\xb8\\xd0\\xb7\\xd0\\xb5\\xd0\\xbc\\xd0\\xbd\\xd0\\xbe\\xd0\\xbc\\xd0\\xbe\\xd1\\x80\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd1\\x82\\xd0\\xbe \\xd1\\x81\\xd0\\xbf\\xd0\\xbe\\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd1\\x83\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb5\\xd0\\xb6\\xd0\\xb4\\xd1\\x83 \\xd0\\x95\\xd0\\x9e \\xd0\\xb8 \\xd0\\x99\\xd0\\xbe\\xd1\\x80\\xd0\\xb4\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f \\xd1\\x81 \\xd0\\xbe\\xd0\\xb3\\xd0\\xbb\\xd0\\xb5\\xd0\\xb4 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd1\\x81\\xd1\\x8a\\xd0\\xb5\\xd0\\xb4\\xd0\\xb8\\xd0\\xbd\\xd1\\x8f\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb5\\xd1\\x82\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0 \\xd0\\x91\\xd1\\x8a\\xd0\\xbb\\xd0\\xb3\\xd0\\xb0\\xd1\\x80\\xd0\\xb8\\xd1\\x8f \\xd0\\xb8 \\xd0\\xa0\\xd1\\x83\\xd0\\xbc\\xd1\\x8a\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f \\xd0\\xba\\xd1\\x8a\\xd0\\xbc \\xd0\\x95\\xd0\\xa1 (', 'bg')\n",
      "('Closure of the sitting', 'en')\n",
      "('1. Financial year 2008 as modified by the Council (vote) ', 'en')\n",
      "('\\xc4\\x8cas glasovanja', 'sl')\n",
      "('Sklepi o nekaterih dokumentih: glej zapisnik', 'sl')\n"
     ]
    }
   ],
   "source": [
    "# Check data sample\n",
    "import random\n",
    "rand_indices = random.sample(range(len(data)),  20)\n",
    "for i in rand_indices:\n",
    "    print(data[i],labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aftale mellem EF og japans lin p\\xc3\\xb6yt\\xc3\\xa4kirjan hyv\\xc3\\xa4ksyminen '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "test_text = \"12. Aftale mellem EF\\\" 'og' Japans !2### ( 100.9 lin 1233 ??? p\\xc3\\xb6yt\\xc3\\xa4kirjan hyv\\xc3\\xa4ksyminen:\"\n",
    "utils.preprocess(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 31.3 ms, total: 1.37 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(data)):\n",
    "    data[i] = utils.preprocess(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predlog splo\\xc5\\xa1nega prora\\xc4\\x8duna za leto oddelek III ', '\\xc5\\xbdenske in vodenje podjetij ', 'razmere na bli\\xc5\\xbenjem vzhodugaza glasovanje ', 'predlo\\xc5\\xbeitev dokumentov glej zapisnik ', 'javna ponudba vrednostnih papirjev in uskladitev zahtev v zvezi s preglednostjo razprava ']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lang_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for token in data[i].split():\n",
    "        if (labels[i] in lang_dict) and (token in lang_dict[labels[i]]):\n",
    "            lang_dict[labels[i]][token] += 1\n",
    "        else:\n",
    "            lang_dict[labels[i]][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict['en']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 6713)\n",
      "('fr', 4738)\n",
      "('bg', 5449)\n",
      "('nl', 5100)\n",
      "('ro', 5111)\n",
      "('pt', 4241)\n",
      "('lv', 6186)\n",
      "('sv', 5645)\n",
      "('de', 5617)\n",
      "('it', 4819)\n",
      "('hu', 6841)\n",
      "('sk', 6480)\n",
      "('et', 6733)\n",
      "('lt', 6431)\n",
      "('en', 4035)\n",
      "('pl', 6485)\n",
      "('sl', 6391)\n",
      "('cs', 6002)\n",
      "('fi', 7015)\n",
      "('da', 5240)\n",
      "('es', 4273)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size in each language\n",
    "for key in lang_dict.keys():\n",
    "    print(key, len(lang_dict[key].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/lang_dict.json', 'wb') as outfile:\n",
    "    json.dump(lang_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create n-gram train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab abc abc bcd abc bcd cde abc bcd cde def\n",
      "ab abc abcd abcd bcde abcd bcde cdef\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "test_text = \"ab abc abcd abcde abcdef\"\n",
    "print(utils.create_n_gram(test_text, 3))\n",
    "print(utils.create_n_gram(test_text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_n_gram(data[i], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 5224)\n",
      "('fr', 8727)\n",
      "('bg', 3993)\n",
      "('nl', 11132)\n",
      "('ro', 8310)\n",
      "('pt', 8414)\n",
      "('lv', 10040)\n",
      "('sv', 11468)\n",
      "('de', 11788)\n",
      "('it', 7959)\n",
      "('hu', 12743)\n",
      "('sk', 11041)\n",
      "('et', 11157)\n",
      "('lt', 10750)\n",
      "('en', 8192)\n",
      "('pl', 10860)\n",
      "('sl', 9759)\n",
      "('cs', 11008)\n",
      "('fi', 11643)\n",
      "('da', 11128)\n",
      "('es', 8069)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/lang_dict_4_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lang_dict_4_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create combined n-gram and full word train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab abc abcd abc bcd abcde abc bcd cde abcdef abc bcd cde def\n",
      "ab abc abcd abcde abcd bcde abcdef abcd bcde cdef\n"
     ]
    }
   ],
   "source": [
    "test_text = \"ab abc abcd abcde abcdef\"\n",
    "print(utils.create_full_word_and_n_gram(test_text, 3))\n",
    "print(utils.create_full_word_and_n_gram(test_text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11703\n",
      "5324\n",
      "('el', 11711)\n",
      "('fr', 13030)\n",
      "('bg', 9304)\n",
      "('nl', 15864)\n",
      "('ro', 13051)\n",
      "('pt', 12239)\n",
      "('lv', 15893)\n",
      "('sv', 16726)\n",
      "('de', 17082)\n",
      "('it', 12348)\n",
      "('hu', 19245)\n",
      "('sk', 17138)\n",
      "('et', 17583)\n",
      "('lt', 16888)\n",
      "('en', 11703)\n",
      "('pl', 16940)\n",
      "('sl', 15699)\n",
      "('cs', 16629)\n",
      "('fi', 18424)\n",
      "('da', 15995)\n",
      "('es', 11965)\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_n_gram(data[i], 4)\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_full_word_4_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11703\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "with open('data/lang_dict_full_word_4_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create 3-gram, 4-gram, 5-gram and full word train set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where whe her ere wher here is the treasure tre rea eas asu sur ure trea reas easu asur sure treas reasu easur asure hidden hid idd dde den hidd idde dden hidde idden\n"
     ]
    }
   ],
   "source": [
    "test_text = \"where is the treasure hidden\"\n",
    "print(utils.create_full_word_and_multiple_n_gram(test_text, [3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24208\n",
      "5480\n",
      "('el', 24062)\n",
      "('fr', 27117)\n",
      "('bg', 18528)\n",
      "('nl', 34970)\n",
      "('ro', 26743)\n",
      "('pt', 25531)\n",
      "('lv', 32789)\n",
      "('sv', 36119)\n",
      "('de', 37694)\n",
      "('it', 25305)\n",
      "('hu', 41441)\n",
      "('sk', 35545)\n",
      "('et', 37241)\n",
      "('lt', 35389)\n",
      "('en', 24208)\n",
      "('pl', 35068)\n",
      "('sl', 31510)\n",
      "('cs', 34704)\n",
      "('fi', 40213)\n",
      "('da', 34847)\n",
      "('es', 24752)\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_multiple_n_gram(data[i], [3,4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_full_word_3_4_5_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24208\n",
      "5480\n"
     ]
    }
   ],
   "source": [
    "with open('data/lang_dict_full_word_3_4_5_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create 4-5 gram and full word model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20338\n",
      "5324\n",
      "('el', 20765)\n",
      "('fr', 23072)\n",
      "('bg', 15955)\n",
      "('nl', 30176)\n",
      "('ro', 23002)\n",
      "('pt', 21631)\n",
      "('lv', 28159)\n",
      "('sv', 31235)\n",
      "('de', 32683)\n",
      "('it', 21933)\n",
      "('hu', 35591)\n",
      "('sk', 30423)\n",
      "('et', 32916)\n",
      "('lt', 30800)\n",
      "('en', 20338)\n",
      "('pl', 30223)\n",
      "('sl', 27296)\n",
      "('cs', 29412)\n",
      "('fi', 35776)\n",
      "('da', 30069)\n",
      "('es', 21052)\n",
      "20338\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_multiple_n_gram(data[i], [4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_full_word_4_5_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)\n",
    "    \n",
    "with open('data/lang_dict_full_word_4_5_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
