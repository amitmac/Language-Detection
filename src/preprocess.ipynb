{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sl', 'es', 'el', 'nl', 'hu', 'it', 'bg', 'sk', 'da', 'sv', 'cs', 'lt', 'de', 'en', 'pl', 'fr', 'fi', 'lv', 'pt', 'et', 'ro']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/lang-detect/txt/\"\n",
    "dir_list = os.listdir(data_path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) > 1:\n",
    "            return lines[1].strip(\"\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.53 s, sys: 7.54 s, total: 14.1 s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data, labels = [], []\n",
    "for dir_name in dir_list:\n",
    "    files_list = os.listdir(data_path + dir_name)\n",
    "    for f in files_list:\n",
    "        sent = read_data(data_path + dir_name + \"/\" + f)\n",
    "        if sent:\n",
    "            data.append(sent)\n",
    "            labels.append(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of data', 186458)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of data\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3. Burma (afstemning) ', 'da')\n",
      "('5. Dispositifs de remorquage et de marche arri\\xc3\\xa8re des tracteurs agricoles ou forestiers \\xc3\\xa0 roues (Version codifi\\xc3\\xa9e) (codified version) (vote) ', 'fr')\n",
      "('4. B\\xc4\\x9blorusko (hlasov\\xc3\\xa1n\\xc3\\xad) ', 'cs')\n",
      "('5. Elintarvikkeiden hintojen nousu Euroopan unionissa ja kehitysmaissa (\\xc3\\xa4\\xc3\\xa4nestys) ', 'fi')\n",
      "('8. Az energiapiac integrit\\xc3\\xa1sa \\xc3\\xa9s \\xc3\\xa1tl\\xc3\\xa1that\\xc3\\xb3s\\xc3\\xa1ga (', 'hu')\n",
      "('The impact of cohesion policy on the integration of vulnerable communities and groups (debate) ', 'en')\n",
      "('15. Posilnenie chemickej, biologickej, r\\xc3\\xa1diologickej a jadrovej bezpe\\xc4\\x8dnosti v Eur\\xc3\\xb3pskej \\xc3\\xbanii - ak\\xc4\\x8dn\\xc3\\xbd pl\\xc3\\xa1n E\\xc3\\x9a v oblasti CBRN bezpe\\xc4\\x8dnosti (', 'sk')\n",
      "('Egyperces felsz\\xc3\\xb3lal\\xc3\\xa1sok jelent\\xc5\\x91s politikai k\\xc3\\xa9rd\\xc3\\xa9sekben', 'hu')\n",
      "('7. Projecto de or\\xc3\\xa7amento rectificativo n.\\xc2\\xba 4/2010: Sec\\xc3\\xa7\\xc3\\xa3o III - Comiss\\xc3\\xa3o (Excedente de 2009) (', 'pt')\n",
      "('J\\xc3\\xa4rgmiste istungite ajakava (vaata protokolli)', 'et')\n",
      "('8. Sept\\xc4\\xabtais un Astotais ikgad\\xc4\\x93jais zi\\xc5\\x86ojums par iero\\xc4\\x8du eksportu (balsojums)', 'lv')\n",
      "('27. Decharge 2008: Det Europ\\xc3\\xa6iske Agentur for Grundl\\xc3\\xa6ggende Rettigheder', 'da')\n",
      "('Gennemf\\xc3\\xb8relsesforanstaltninger (forretningsordenens artikel 88): se protokollen', 'da')\n",
      "('1. Prora\\xc4\\x8dunske smernice : 2011 - drugi oddelki (', 'sl')\n",
      "('Bambagysl\\xc4\\x97s kraujo kamienin\\xc4\\x97s l\\xc4\\x85stel\\xc4\\x97s (diskusijos) ', 'lt')\n",
      "('Di\\xc3\\xa1logo activo com os cidad\\xc3\\xa3os sobre a Europa (breve apresenta\\xc3\\xa7\\xc3\\xa3o) ', 'pt')\n",
      "('Istungil vastuv\\xc3\\xb5etud tekstide edastamine (vt protokoll)', 'et')\n",
      "('Balsojuma skaidrojums', 'lv')\n",
      "('Ben\\xc3\\xa4mningar p\\xc3\\xa5 textilier och m\\xc3\\xa4rkning av textilprodukter (debatt)', 'sv')\n",
      "('Pet\\xc3\\xadci\\xc3\\xb3k: l\\xc3\\xa1sd a jegyz\\xc5\\x91k\\xc3\\xb6nyvet', 'hu')\n"
     ]
    }
   ],
   "source": [
    "# Check data sample\n",
    "import random\n",
    "rand_indices = random.sample(range(len(data)),  20)\n",
    "for i in rand_indices:\n",
    "    print(data[i],labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aftale mellem EF og japans lin p\\xc3\\xb6yt\\xc3\\xa4kirjan hyv\\xc3\\xa4ksyminen '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "test_text = \"12. Aftale mellem EF\\\" 'og' Japans !2### ( 100.9 lin 1233 ??? p\\xc3\\xb6yt\\xc3\\xa4kirjan hyv\\xc3\\xa4ksyminen:\"\n",
    "utils.preprocess(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 94.5 ms, total: 1.36 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(data)):\n",
    "    data[i] = utils.preprocess(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predlog splo\\xc5\\xa1nega prora\\xc4\\x8duna za leto oddelek III ', '\\xc5\\xbdenske in vodenje podjetij ', 'razmere na bli\\xc5\\xbenjem vzhodugaza glasovanje ', 'predlo\\xc5\\xbeitev dokumentov glej zapisnik ', 'javna ponudba vrednostnih papirjev in uskladitev zahtev v zvezi s preglednostjo razprava ']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lang_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for token in data[i].split():\n",
    "        if (labels[i] in lang_dict) and (token in lang_dict[labels[i]]):\n",
    "            lang_dict[labels[i]][token] += 1\n",
    "        else:\n",
    "            lang_dict[labels[i]][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict['en']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 6713)\n",
      "('fr', 4738)\n",
      "('bg', 5449)\n",
      "('nl', 5100)\n",
      "('ro', 5111)\n",
      "('pt', 4241)\n",
      "('lv', 6186)\n",
      "('sv', 5645)\n",
      "('de', 5617)\n",
      "('it', 4819)\n",
      "('hu', 6841)\n",
      "('sk', 6480)\n",
      "('et', 6733)\n",
      "('lt', 6431)\n",
      "('en', 4035)\n",
      "('pl', 6485)\n",
      "('sl', 6391)\n",
      "('cs', 6002)\n",
      "('fi', 7015)\n",
      "('da', 5240)\n",
      "('es', 4273)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size in each language\n",
    "for key in lang_dict.keys():\n",
    "    print(key, len(lang_dict[key].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/lang_dict.json', 'wb') as outfile:\n",
    "    json.dump(lang_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create n-gram train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab abc abc bcd abc bcd cde abc bcd cde def\n",
      "ab abc abcd abcd bcde abcd bcde cdef\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "test_text = \"ab abc abcd abcde abcdef\"\n",
    "print(utils.create_n_gram(test_text, 3))\n",
    "print(utils.create_n_gram(test_text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_n_gram(data[i], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 5224)\n",
      "('fr', 8727)\n",
      "('bg', 3993)\n",
      "('nl', 11132)\n",
      "('ro', 8310)\n",
      "('pt', 8414)\n",
      "('lv', 10040)\n",
      "('sv', 11468)\n",
      "('de', 11788)\n",
      "('it', 7959)\n",
      "('hu', 12743)\n",
      "('sk', 11041)\n",
      "('et', 11157)\n",
      "('lt', 10750)\n",
      "('en', 8192)\n",
      "('pl', 10860)\n",
      "('sl', 9759)\n",
      "('cs', 11008)\n",
      "('fi', 11643)\n",
      "('da', 11128)\n",
      "('es', 8069)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/lang_dict_4_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lang_dict_4_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create combined n-gram and full word train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab abc abcd abc bcd abcde abc bcd cde abcdef abc bcd cde def\n",
      "ab abc abcd abcde abcd bcde abcdef abcd bcde cdef\n"
     ]
    }
   ],
   "source": [
    "test_text = \"ab abc abcd abcde abcdef\"\n",
    "print(utils.create_full_word_and_n_gram(test_text, 3))\n",
    "print(utils.create_full_word_and_n_gram(test_text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11703\n",
      "5324\n",
      "('el', 11711)\n",
      "('fr', 13030)\n",
      "('bg', 9304)\n",
      "('nl', 15864)\n",
      "('ro', 13051)\n",
      "('pt', 12239)\n",
      "('lv', 15893)\n",
      "('sv', 16726)\n",
      "('de', 17082)\n",
      "('it', 12348)\n",
      "('hu', 19245)\n",
      "('sk', 17138)\n",
      "('et', 17583)\n",
      "('lt', 16888)\n",
      "('en', 11703)\n",
      "('pl', 16940)\n",
      "('sl', 15699)\n",
      "('cs', 16629)\n",
      "('fi', 18424)\n",
      "('da', 15995)\n",
      "('es', 11965)\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_n_gram(data[i], 4)\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_full_word_4_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11703\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "with open('data/lang_dict_full_word_4_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create 3-gram, 4-gram, 5-gram and full word train set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where whe her ere wher here is the treasure tre rea eas asu sur ure trea reas easu asur sure treas reasu easur asure hidden hid idd dde den hidd idde dden hidde idden\n"
     ]
    }
   ],
   "source": [
    "test_text = \"where is the treasure hidden\"\n",
    "print(utils.create_full_word_and_multiple_n_gram(test_text, [3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24208\n",
      "5480\n",
      "('el', 24062)\n",
      "('fr', 27117)\n",
      "('bg', 18528)\n",
      "('nl', 34970)\n",
      "('ro', 26743)\n",
      "('pt', 25531)\n",
      "('lv', 32789)\n",
      "('sv', 36119)\n",
      "('de', 37694)\n",
      "('it', 25305)\n",
      "('hu', 41441)\n",
      "('sk', 35545)\n",
      "('et', 37241)\n",
      "('lt', 35389)\n",
      "('en', 24208)\n",
      "('pl', 35068)\n",
      "('sl', 31510)\n",
      "('cs', 34704)\n",
      "('fi', 40213)\n",
      "('da', 34847)\n",
      "('es', 24752)\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_multiple_n_gram(data[i], [3,4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_full_word_3_4_5_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24208\n",
      "5480\n"
     ]
    }
   ],
   "source": [
    "with open('data/lang_dict_full_word_3_4_5_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create 4-5 gram and full word model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20338\n",
      "5324\n",
      "('el', 20765)\n",
      "('fr', 23072)\n",
      "('bg', 15955)\n",
      "('nl', 30176)\n",
      "('ro', 23002)\n",
      "('pt', 21631)\n",
      "('lv', 28159)\n",
      "('sv', 31235)\n",
      "('de', 32683)\n",
      "('it', 21933)\n",
      "('hu', 35591)\n",
      "('sk', 30423)\n",
      "('et', 32916)\n",
      "('lt', 30800)\n",
      "('en', 20338)\n",
      "('pl', 30223)\n",
      "('sl', 27296)\n",
      "('cs', 29412)\n",
      "('fi', 35776)\n",
      "('da', 30069)\n",
      "('es', 21052)\n",
      "20338\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_multiple_n_gram(data[i], [4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_full_word_4_5_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)\n",
    "    \n",
    "with open('data/lang_dict_full_word_4_5_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create multiwords key dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217868\n",
      "5480\n",
      "('el', 307666)\n",
      "('fr', 252892)\n",
      "('bg', 235303)\n",
      "('nl', 309236)\n",
      "('ro', 256304)\n",
      "('pt', 234432)\n",
      "('lv', 315895)\n",
      "('sv', 321174)\n",
      "('de', 334457)\n",
      "('it', 251587)\n",
      "('hu', 378673)\n",
      "('sk', 329805)\n",
      "('et', 351870)\n",
      "('lt', 340465)\n",
      "('en', 217868)\n",
      "('pl', 333695)\n",
      "('sl', 303643)\n",
      "('cs', 316170)\n",
      "('fi', 384354)\n",
      "('da', 301523)\n",
      "('es', 230871)\n",
      "CPU times: user 1min 23s, sys: 798 ms, total: 1min 24s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = utils.create_full_word_and_multiple_n_gram(data[i], [3,4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    d = new_data[i].split()\n",
    "    for j in range(len(d)-1):\n",
    "        token = d[j] + '_' + d[j+1]\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    d = new_data[i].split()\n",
    "    for j in range(len(d)-2):\n",
    "        token = d[j] + '_' + d[j+1] + '_' + d[j+2]\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "            \n",
    "for i in range(len(new_data)):\n",
    "    d = new_data[i].split()\n",
    "    for j in range(len(d)-3):\n",
    "        token = d[j] + '_' + d[j+1] + '_' + d[j+2] + '_' + d[j+3]\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "            \n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('data/lang_dict_multiword_4_gram_key_model.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
