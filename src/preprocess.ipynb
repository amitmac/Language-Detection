{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sl', 'es', 'el', 'nl', 'hu', 'it', 'bg', 'sk', 'da', 'sv', 'cs', 'lt', 'de', 'en', 'pl', 'fr', 'fi', 'lv', 'pt', 'et', 'ro']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/lang-detect/txt/\"\n",
    "dir_list = os.listdir(data_path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) > 1:\n",
    "            return lines[1].strip(\"\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.9 s, sys: 5.6 s, total: 10.5 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data, labels = [], []\n",
    "for dir_name in dir_list:\n",
    "    files_list = os.listdir(data_path + dir_name)\n",
    "    for f in files_list:\n",
    "        sent = read_data(data_path + dir_name + \"/\" + f)\n",
    "        if sent:\n",
    "            data.append(sent)\n",
    "            labels.append(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of data', 186458)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of data\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hodina ot\\xc3\\xa1zok (ot\\xc3\\xa1zky pre Radu)', 'sk')\n",
      "('9. Beschikbaarstelling van middelen uit het Europees Fonds voor aanpassing aan de globalisering: Lear/Spanje (', 'nl')\n",
      "('EU-Turkey relations (debate) ', 'en')\n",
      "('2. Struktury zarz\\xc4\\x85dzania europejskimi programami radionawigacji satelitarnej (', 'pl')\n",
      "('Onderzoek geloofsbrieven: zie notulen', 'nl')\n",
      "('22. Aprobaci\\xc3\\xb3n de la gesti\\xc3\\xb3n 2006: Centro Europeo para la Prevenci\\xc3\\xb3n y el Control de las Enfermedades (', 'es')\n",
      "('Beziehungen EU/Tunesien (Aussprache)', 'de')\n",
      "('Chiusura della seduta', 'it')\n",
      "('7. Az \\xc3\\xa9lelmiszerek \\xc3\\xa9s a takarm\\xc3\\xa1nyok radioakt\\xc3\\xadv szennyezetts\\xc3\\xa9g\\xc3\\xa9nek legmagasabb megengedhet\\xc5\\x91 hat\\xc3\\xa1r\\xc3\\xa9rt\\xc3\\xa9ke (kodifik\\xc3\\xa1lt v\\xc3\\xa1ltozat) (szavaz\\xc3\\xa1s) ', 'hu')\n",
      "('1. Az Eur\\xc3\\xb3pai Globaliz\\xc3\\xa1ci\\xc3\\xb3s Alkalmazkod\\xc3\\xa1si Alap ig\\xc3\\xa9nybev\\xc3\\xa9tele: \\xc3\\x8drorsz\\xc3\\xa1g - SR Technics (', 'hu')\n",
      "('Tekster til aftaler sendt af R\\xc3\\xa5det: se protokollen', 'da')\n",
      "('Balsavimo pataisymai ir ketinimai (\\xc5\\xber. protokol\\xc4\\x85)', 'lt')\n",
      "('\\xce\\x94\\xce\\xb9\\xce\\xb1\\xcf\\x84\\xce\\xbb\\xce\\xb1\\xce\\xbd\\xcf\\x84\\xce\\xb9\\xce\\xba\\xcf\\x8c \\xce\\x9f\\xce\\xb9\\xce\\xba\\xce\\xbf\\xce\\xbd\\xce\\xbf\\xce\\xbc\\xce\\xb9\\xce\\xba\\xcf\\x8c \\xce\\xa3\\xcf\\x85\\xce\\xbc\\xce\\xb2\\xce\\xbf\\xcf\\x8d\\xce\\xbb\\xce\\xb9\\xce\\xbf (\\xcf\\x83\\xcf\\x85\\xce\\xb6\\xce\\xae\\xcf\\x84\\xce\\xb7\\xcf\\x83\\xce\\xb7)', 'el')\n",
      "('10. Absolut\\xc3\\xb3rium za rok 2007: Eur\\xc3\\xb3pske centrum pre prevenciu a kontrolu chor\\xc3\\xb4b (', 'sk')\n",
      "('9.  Eiropas kaimi\\xc5\\x86attiec\\xc4\\xabbu politikas p\\xc4\\x81rskat\\xc4\\xab\\xc5\\xa1ana - Dienvidu dimensija (', 'lv')\n",
      "('Godkendelse af protokollen fra foreg\\xc3\\xa5ende m\\xc3\\xb8de: se protokollen', 'da')\n",
      "('1. Kalend\\xc3\\xa1\\xc5\\x99 plen\\xc3\\xa1rn\\xc3\\xadch zased\\xc3\\xa1n\\xc3\\xad Evropsk\\xc3\\xa9ho parlamentu 2010', 'cs')\n",
      "('22. Tausiojo pesticid\\xc5\\xb3 naudojimo teminstrategija (balsavimas)', 'lt')\n",
      "('19. Medlemsstaternas m\\xc3\\xb6jlighet att begr\\xc3\\xa4nsa eller f\\xc3\\xb6rbjuda odling av genetiskt modifierade organismer inom sina territorier (', 'sv')\n",
      "(\"1. Instrument europ\\xc3\\xa9en de microfinancement en faveur de l'emploi et de l'inclusion (Progress) (\", 'fr')\n"
     ]
    }
   ],
   "source": [
    "# Check data sample\n",
    "import random\n",
    "rand_indices = random.sample(range(len(data)),  20)\n",
    "for i in rand_indices:\n",
    "    print(data[i],labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aftale mellem EF og japans lin p\\xc3\\xb6yt\\xc3\\xa4kirjan hyv\\xc3\\xa4ksyminen '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "test_text = \"12. Aftale mellem EF\\\" 'og' Japans !2### ( 100.9 lin 1233 ??? p\\xc3\\xb6yt\\xc3\\xa4kirjan hyv\\xc3\\xa4ksyminen:\"\n",
    "utils.preprocess(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 79.2 ms, total: 1.38 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(data)):\n",
    "    data[i] = utils.preprocess(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predlog splo\\xc5\\xa1nega prora\\xc4\\x8duna za leto oddelek III ', '\\xc5\\xbdenske in vodenje podjetij ', 'razmere na bli\\xc5\\xbenjem vzhodugaza glasovanje ', 'predlo\\xc5\\xbeitev dokumentov glej zapisnik ', 'javna ponudba vrednostnih papirjev in uskladitev zahtev v zvezi s preglednostjo razprava ']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lang_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for token in data[i].split():\n",
    "        if (labels[i] in lang_dict) and (token in lang_dict[labels[i]]):\n",
    "            lang_dict[labels[i]][token] += 1\n",
    "        else:\n",
    "            lang_dict[labels[i]][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict['en']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 6713)\n",
      "('fr', 4738)\n",
      "('bg', 5449)\n",
      "('nl', 5100)\n",
      "('ro', 5111)\n",
      "('pt', 4241)\n",
      "('lv', 6186)\n",
      "('sv', 5645)\n",
      "('de', 5617)\n",
      "('it', 4819)\n",
      "('hu', 6841)\n",
      "('sk', 6480)\n",
      "('et', 6733)\n",
      "('lt', 6431)\n",
      "('en', 4035)\n",
      "('pl', 6485)\n",
      "('sl', 6391)\n",
      "('cs', 6002)\n",
      "('fi', 7015)\n",
      "('da', 5240)\n",
      "('es', 4273)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size in each language\n",
    "for key in lang_dict.keys():\n",
    "    print(key, len(lang_dict[key].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lang_dict.json', 'wb') as outfile:\n",
    "    json.dump(lang_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create n-gram train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab abc abc bcd abc bcd cde abc bcd cde def\n",
      "ab abc abcd abcd bcde abcd bcde cdef\n"
     ]
    }
   ],
   "source": [
    "def create_n_gram(text, n):\n",
    "    new_string = \"\"\n",
    "    for token in text.split():\n",
    "        if len(token) <= n:\n",
    "            new_string += (token + \" \")\n",
    "        else:\n",
    "            for i in range(len(token)-n+1):\n",
    "                new_string += (token[i:i+n] + \" \")\n",
    "    return new_string.strip()\n",
    "\n",
    "test_text = \"ab abc abcd abcde abcdef\"\n",
    "print(create_n_gram(test_text, 3))\n",
    "print(create_n_gram(test_text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = create_n_gram(data[i], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 5224)\n",
      "('fr', 8727)\n",
      "('bg', 3993)\n",
      "('nl', 11132)\n",
      "('ro', 8310)\n",
      "('pt', 8414)\n",
      "('lv', 10040)\n",
      "('sv', 11468)\n",
      "('de', 11788)\n",
      "('it', 7959)\n",
      "('hu', 12743)\n",
      "('sk', 11041)\n",
      "('et', 11157)\n",
      "('lt', 10750)\n",
      "('en', 8192)\n",
      "('pl', 10860)\n",
      "('sl', 9759)\n",
      "('cs', 11008)\n",
      "('fi', 11643)\n",
      "('da', 11128)\n",
      "('es', 8069)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lang_dict_4_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lang_dict_4_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create combined n-gram and full word train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab abc abcd abc bcd abcde abc bcd cde abcdef abc bcd cde def\n",
      "ab abc abcd abcde abcd bcde abcdef abcd bcde cdef\n"
     ]
    }
   ],
   "source": [
    "def create_full_word_and_n_gram(text, n):\n",
    "    new_string = \"\"\n",
    "    for token in text.split():\n",
    "        new_string += (token + \" \")\n",
    "        if len(token) > n:\n",
    "            for i in range(len(token)-n+1):\n",
    "                new_string += (token[i:i+n] + \" \")\n",
    "    return new_string.strip()\n",
    "\n",
    "test_text = \"ab abc abcd abcde abcdef\"\n",
    "print(create_full_word_and_n_gram(test_text, 3))\n",
    "print(create_full_word_and_n_gram(test_text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11703\n",
      "5324\n",
      "('el', 11711)\n",
      "('fr', 13030)\n",
      "('bg', 9304)\n",
      "('nl', 15864)\n",
      "('ro', 13051)\n",
      "('pt', 12239)\n",
      "('lv', 15893)\n",
      "('sv', 16726)\n",
      "('de', 17082)\n",
      "('it', 12348)\n",
      "('hu', 19245)\n",
      "('sk', 17138)\n",
      "('et', 17583)\n",
      "('lt', 16888)\n",
      "('en', 11703)\n",
      "('pl', 16940)\n",
      "('sl', 15699)\n",
      "('cs', 16629)\n",
      "('fi', 18424)\n",
      "('da', 15995)\n",
      "('es', 11965)\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = create_full_word_and_n_gram(data[i], 4)\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('lang_dict_full_word_4_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11703\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "with open('lang_dict_full_word_4_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create 3-gram, 4-gram, 5-gram and full word train set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where whe her ere wher here is the treasure tre rea eas asu sur ure trea reas easu asur sure treas reasu easur asure hidden hid idd dde den hidd idde dden hidde idden\n"
     ]
    }
   ],
   "source": [
    "def create_full_word_and_multiple_n_gram(text, n_gram_list):\n",
    "    new_string = \"\"\n",
    "    for token in text.split():\n",
    "        new_string += (token + \" \")\n",
    "        for n in n_gram_list:\n",
    "            if len(token) > n:\n",
    "                for i in range(len(token)-n+1):\n",
    "                    new_string += (token[i:i+n] + \" \")\n",
    "    return new_string.strip()\n",
    "\n",
    "test_text = \"where is the treasure hidden\"\n",
    "print(create_full_word_and_multiple_n_gram(test_text, [3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24208\n",
      "5480\n",
      "('el', 24062)\n",
      "('fr', 27117)\n",
      "('bg', 18528)\n",
      "('nl', 34970)\n",
      "('ro', 26743)\n",
      "('pt', 25531)\n",
      "('lv', 32789)\n",
      "('sv', 36119)\n",
      "('de', 37694)\n",
      "('it', 25305)\n",
      "('hu', 41441)\n",
      "('sk', 35545)\n",
      "('et', 37241)\n",
      "('lt', 35389)\n",
      "('en', 24208)\n",
      "('pl', 35068)\n",
      "('sl', 31510)\n",
      "('cs', 34704)\n",
      "('fi', 40213)\n",
      "('da', 34847)\n",
      "('es', 24752)\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = create_full_word_and_multiple_n_gram(data[i], [3,4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('lang_dict_full_word_3_4_5_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24208\n",
      "5480\n"
     ]
    }
   ],
   "source": [
    "with open('lang_dict_full_word_3_4_5_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create 4-5 gram and full word model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20338\n",
      "5324\n",
      "('el', 20765)\n",
      "('fr', 23072)\n",
      "('bg', 15955)\n",
      "('nl', 30176)\n",
      "('ro', 23002)\n",
      "('pt', 21631)\n",
      "('lv', 28159)\n",
      "('sv', 31235)\n",
      "('de', 32683)\n",
      "('it', 21933)\n",
      "('hu', 35591)\n",
      "('sk', 30423)\n",
      "('et', 32916)\n",
      "('lt', 30800)\n",
      "('en', 20338)\n",
      "('pl', 30223)\n",
      "('sl', 27296)\n",
      "('cs', 29412)\n",
      "('fi', 35776)\n",
      "('da', 30069)\n",
      "('es', 21052)\n",
      "20338\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "new_data = [0]*len(data)\n",
    "for i in range(len(data)):\n",
    "    new_data[i] = create_full_word_and_multiple_n_gram(data[i], [4,5])\n",
    "\n",
    "lang_dict_n_gram = {}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for token in new_data[i].split():\n",
    "        if (labels[i] in lang_dict_n_gram) and (token in lang_dict_n_gram[labels[i]]):\n",
    "            lang_dict_n_gram[labels[i]][token] += 1\n",
    "        else:\n",
    "            if not (labels[i] in lang_dict_n_gram):\n",
    "                lang_dict_n_gram[labels[i]] = {}\n",
    "            lang_dict_n_gram[labels[i]][token] = 0\n",
    "\n",
    "print(len(lang_dict_n_gram['en'].keys()))\n",
    "print(lang_dict_n_gram['en']['the'])\n",
    "\n",
    "# Vocabulary size in each language\n",
    "for key in lang_dict_n_gram.keys():\n",
    "    print(key, len(lang_dict_n_gram[key].keys()))\n",
    "    \n",
    "import pickle\n",
    "with open('lang_dict_full_word_4_5_gram.json', 'w') as f:\n",
    "    pickle.dump(lang_dict_n_gram, f)\n",
    "    \n",
    "with open('lang_dict_full_word_4_5_gram.json', 'r') as f:\n",
    "    ld = pickle.load(f)\n",
    "\n",
    "print(len(ld['en'].keys()))\n",
    "print(ld['en']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
